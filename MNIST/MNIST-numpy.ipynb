{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用NumPy搭建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型抽象类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self) -> None:\n",
    "        super(Module, self).__init__()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.forward(*args, **kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=False) -> None:\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = np.random.normal(size=(in_features, out_features))\n",
    "        self.X = None\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = np.random.normal(size=(out_features))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        Y = np.dot(X, self.W)\n",
    "        if self.bias is not None:\n",
    "            Y = Y + self.bias\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, delta_Y, lr):\n",
    "        \"\"\"\n",
    "        delta_Y: (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        delta_Y_ = np.dot(delta_Y, self.W.transpose())\n",
    "        self.W = self.W - np.dot(self.X.transpose(), delta_Y) * lr\n",
    "        if self.bias is not None:\n",
    "            self.bias -= np.average(delta_Y, axis=0)\n",
    "        return delta_Y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh激活函数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Tanh, self).__init__()\n",
    "        self.Y = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Y = np.tanh(X)\n",
    "        return self.Y\n",
    "\n",
    "    def backward(self, delta_Y):\n",
    "        return np.multiply((1 - self.Y ** 2), delta_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Softmax, self).__init__()\n",
    "        self.exps = None\n",
    "        self.exps_sum = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch_size, features)\n",
    "        \"\"\"\n",
    "        C = np.max(X)\n",
    "        self.exps = np.exp(X - C)\n",
    "        self.exps_sum = np.sum(self.exps, axis=1).reshape((-1, 1))\n",
    "        return np.divide(self.exps, self.exps_sum)\n",
    "\n",
    "    def backward(self, delta_Y):\n",
    "        \"\"\"\n",
    "        delta_Y: (batch_size, features)\n",
    "        \"\"\"\n",
    "        exps_sum_square = self.exps_sum ** 2\n",
    "        ii_matrix = np.multiply(self.exps, self.exps_sum) / exps_sum_square # (batch_size, features)\n",
    "        ij_matrix = - np.matmul(self.exps[:, :, np.newaxis], self.exps[:, np.newaxis, :]) / exps_sum_square[:, :, np.newaxis] # (batch_size, features, features)\n",
    "        ij_Y = np.multiply(delta_Y[:, :, np.newaxis], ij_matrix).sum(axis=1) # (batch_size, features)\n",
    "        delta_Y = ij_Y + np.multiply(delta_Y, ii_matrix) # (batch_size, features)\n",
    "        return delta_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log对数函数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Log, self).__init__()\n",
    "        self.inf = 1e-10\n",
    "        self.X = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.log(X + self.inf)\n",
    "    \n",
    "    def backward(self, delta_Y):\n",
    "        return np.multiply(1 / (self.X + self.inf), delta_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLloss(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NLLloss, self).__init__()\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "    \n",
    "    def forward(self, Y, target):\n",
    "        \"\"\"\n",
    "        Y: (batch_size, features)\n",
    "        target: (batch_size)\n",
    "        \"\"\"\n",
    "        self.target = np.ones(shape=Y.shape) * 1e-6\n",
    "        for i, j in enumerate(target):\n",
    "            self.target[i, j] = 1\n",
    "        self.loss = -np.sum(np.multiply(Y, self.target)) / len(target)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        return - self.target * self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# 打乱训练集\n",
    "index = list(range(len(X_train)))\n",
    "np.random.shuffle(index)\n",
    "X_train = X_train[index]\n",
    "y_train = y_train[index]\n",
    "\n",
    "index = list(range(len(X_test)))\n",
    "np.random.shuffle(index)\n",
    "X_test = X_test[index]\n",
    "y_test = y_test[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "lr = 0.0003\n",
    "\n",
    "# 模型\n",
    "linear1 = Linear(in_features=28 * 28, out_features=1024, bias=True)\n",
    "tanh1 = Tanh()\n",
    "linear2 = Linear(in_features=1024, out_features=10, bias=True)\n",
    "softmax = Softmax()\n",
    "log = Log()\n",
    "nllloss = NLLloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:1 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 71.38it/s, loss: 1.0747  Acc:93.75%]\n",
      "Test epoch:1 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.64it/s, Acc [8657/10000 (86.57%)]]\n",
      "Train epoch:2 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 71.95it/s, loss: 0.5686  Acc:93.75%]\n",
      "Test epoch:2 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.57it/s, Acc [8838/10000 (88.38%)]]\n",
      "Train epoch:3 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 71.20it/s, loss: 0.5429  Acc:92.19%]\n",
      "Test epoch:3 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.47it/s, Acc [8911/10000 (89.11%)]]\n",
      "Train epoch:4 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 69.63it/s, loss: 0.5103  Acc:92.19%]\n",
      "Test epoch:4 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.86it/s, Acc [8949/10000 (89.49%)]]\n",
      "Train epoch:5 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 68.80it/s, loss: 0.5022  Acc:93.75%]\n",
      "Test epoch:5 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.15it/s, Acc [8980/10000 (89.80%)]]\n",
      "Train epoch:6 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 68.64it/s, loss: 0.4981  Acc:93.75%]\n",
      "Test epoch:6 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.98it/s, Acc [8994/10000 (89.94%)]]\n",
      "Train epoch:7 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 67.52it/s, loss: 0.4271  Acc:93.75%]\n",
      "Test epoch:7 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.22it/s, Acc [9009/10000 (90.09%)]]\n",
      "Train epoch:8 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 65.89it/s, loss: 0.3694  Acc:93.75%]\n",
      "Test epoch:8 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.90it/s, Acc [9027/10000 (90.27%)]]\n",
      "Train epoch:9 : 100%|█████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 65.19it/s, loss: 0.3076  Acc:93.75%]\n",
      "Test epoch:9 : 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.63it/s, Acc [9032/10000 (90.32%)]]\n",
      "Train epoch:10 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 65.95it/s, loss: 0.2615  Acc:93.75%]\n",
      "Test epoch:10 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.24it/s, Acc [9042/10000 (90.42%)]]\n",
      "Train epoch:11 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:13<00:00, 67.04it/s, loss: 0.2264  Acc:95.31%]\n",
      "Test epoch:11 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.15it/s, Acc [9050/10000 (90.50%)]]\n",
      "Train epoch:12 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 66.16it/s, loss: 0.2034  Acc:95.31%]\n",
      "Test epoch:12 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.32it/s, Acc [9064/10000 (90.64%)]]\n",
      "Train epoch:13 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 64.94it/s, loss: 0.1846  Acc:95.31%]\n",
      "Test epoch:13 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.63it/s, Acc [9061/10000 (90.61%)]]\n",
      "Train epoch:14 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 62.36it/s, loss: 0.1766  Acc:96.88%]\n",
      "Test epoch:14 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.66it/s, Acc [9063/10000 (90.63%)]]\n",
      "Train epoch:15 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 60.15it/s, loss: 0.1816  Acc:96.88%]\n",
      "Test epoch:15 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.68it/s, Acc [9062/10000 (90.62%)]]\n",
      "Train epoch:16 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.29it/s, loss: 0.1830  Acc:96.88%]\n",
      "Test epoch:16 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.95it/s, Acc [9069/10000 (90.69%)]]\n",
      "Train epoch:17 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.15it/s, loss: 0.1776  Acc:96.88%]\n",
      "Test epoch:17 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.95it/s, Acc [9072/10000 (90.72%)]]\n",
      "Train epoch:18 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 59.82it/s, loss: 0.1747  Acc:96.88%]\n",
      "Test epoch:18 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.39it/s, Acc [9071/10000 (90.71%)]]\n",
      "Train epoch:19 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.36it/s, loss: 0.1708  Acc:96.88%]\n",
      "Test epoch:19 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.00it/s, Acc [9070/10000 (90.70%)]]\n",
      "Train epoch:20 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.17it/s, loss: 0.1642  Acc:96.88%]\n",
      "Test epoch:20 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.39it/s, Acc [9071/10000 (90.71%)]]\n",
      "Train epoch:21 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 60.25it/s, loss: 0.1615  Acc:96.88%]\n",
      "Test epoch:21 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.02it/s, Acc [9078/10000 (90.78%)]]\n",
      "Train epoch:22 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 59.21it/s, loss: 0.1611  Acc:96.88%]\n",
      "Test epoch:22 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.99it/s, Acc [9082/10000 (90.82%)]]\n",
      "Train epoch:23 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 60.32it/s, loss: 0.1616  Acc:96.88%]\n",
      "Test epoch:23 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 15.72it/s, Acc [9083/10000 (90.83%)]]\n",
      "Train epoch:24 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:17<00:00, 52.93it/s, loss: 0.1613  Acc:96.88%]\n",
      "Test epoch:24 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 13.77it/s, Acc [9082/10000 (90.82%)]]\n",
      "Train epoch:25 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:16<00:00, 55.17it/s, loss: 0.1630  Acc:98.44%]\n",
      "Test epoch:25 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.09it/s, Acc [9082/10000 (90.82%)]]\n",
      "Train epoch:26 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.58it/s, loss: 0.1656  Acc:98.44%]\n",
      "Test epoch:26 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.97it/s, Acc [9082/10000 (90.82%)]]\n",
      "Train epoch:27 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 63.91it/s, loss: 0.1663  Acc:98.44%]\n",
      "Test epoch:27 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.27it/s, Acc [9082/10000 (90.82%)]]\n",
      "Train epoch:28 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:14<00:00, 66.88it/s, loss: 0.1648  Acc:98.44%]\n",
      "Test epoch:28 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.49it/s, Acc [9086/10000 (90.86%)]]\n",
      "Train epoch:29 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.75it/s, loss: 0.1627  Acc:98.44%]\n",
      "Test epoch:29 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.66it/s, Acc [9088/10000 (90.88%)]]\n",
      "Train epoch:30 : 100%|████████████████████████████████████████████████████████████████████| 937/937 [00:15<00:00, 61.83it/s, loss: 0.1616  Acc:98.44%]\n",
      "Test epoch:30 : 100%|██████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.93it/s, Acc [9088/10000 (90.88%)]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \"\"\" train \"\"\"\n",
    "    process_bar = tqdm(range(len(X_train) // batch_size_train), ncols=150)\n",
    "    for itor in process_bar:\n",
    "        X = X_train[itor * batch_size_train: itor * batch_size_train + batch_size_train]\n",
    "        y = y_train[itor * batch_size_train: itor * batch_size_train + batch_size_train]\n",
    "        \"\"\" 前向传播 \"\"\"\n",
    "        tmp = X.reshape((batch_size_train, -1))\n",
    "        tmp = linear1(tmp)\n",
    "        tmp = tanh1(tmp)\n",
    "        tmp = linear2(tmp)\n",
    "        tmp = softmax(tmp)\n",
    "        pre = np.argmax(tmp, axis=1)\n",
    "\n",
    "        train_acc = np.sum(y == pre)\n",
    "        train_total = len(y)\n",
    "        \n",
    "        tmp = log(tmp)\n",
    "        loss = nllloss(tmp, y)\n",
    "        \"\"\" 反向传播 \"\"\"\n",
    "        Y = nllloss.backward()\n",
    "        Y = log.backward(Y)\n",
    "        Y = softmax.backward(Y)\n",
    "        Y = linear2.backward(Y, lr)\n",
    "        Y = tanh1.backward(Y)\n",
    "        Y = linear1.backward(Y, lr)\n",
    "\n",
    "        process_bar.set_description('Train epoch:{} '.format(epoch + 1))\n",
    "        process_bar.set_postfix_str('loss: {:.4f}  Acc:{:.2f}%'.format(\n",
    "                                    loss, 100. * train_acc / train_total))\n",
    "    \n",
    "    \"\"\" test \"\"\"\n",
    "    test_total = 0\n",
    "    test_acc = 0\n",
    "    test_process_bar = tqdm(range(len(X_test) // batch_size_test), ncols=150)\n",
    "    for itor in test_process_bar:\n",
    "        X = X_test[itor * batch_size_test: itor * batch_size_test + batch_size_test]\n",
    "        y = y_test[itor * batch_size_test: itor * batch_size_test + batch_size_test]\n",
    "\n",
    "        tmp = X.reshape((batch_size_test, -1))\n",
    "        tmp = linear1(tmp)\n",
    "        tmp = tanh1(tmp)\n",
    "        tmp = linear2(tmp)\n",
    "        Y = softmax(tmp)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        test_total += len(y)\n",
    "        test_acc += np.sum(y == Y)\n",
    "        test_process_bar.set_description('Test epoch:{} '.format(epoch + 1))\n",
    "        test_process_bar.set_postfix_str('Acc [{}/{} ({:.2f}%)]'.format(\n",
    "                                        test_acc, test_total, 100. * test_acc/test_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda6d2dda8e0a8f4c399cfaba95f66e67d6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
